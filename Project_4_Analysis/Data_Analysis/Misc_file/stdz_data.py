# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WVLAaEwlGGJE8k8mavY6DtSiKf1YuvMu
"""

import itertools
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.ensemble import VotingClassifier

# File path to the CSV file
file_path = "/content/drive/MyDrive/stdz_data.csv"

# Load the dataset
data = pd.read_csv(file_path)

# Splitting the data into features (X) and target variable (y)
X = data.drop(columns=['Heart Attack Risk'])  # Features
y = data['Heart Attack Risk']  # Target variable

# Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=80)

# Creating individual classifiers
rf_classifier = RandomForestClassifier(n_estimators=50, random_state=80)
gb_classifier = GradientBoostingClassifier(n_estimators=50, random_state=80)
svc_classifier = SVC(kernel='rbf', probability=True, random_state=80)

# Creating a Voting Classifier with different models
voting_classifier = VotingClassifier(estimators=[('Random Forest', rf_classifier),
                                                ('Gradient Boosting', gb_classifier),
                                                ('Support Vector Machine', svc_classifier)],
                                     voting='soft')  # 'soft' for probabilities, 'hard' for majority voting

# Fitting the Voting Classifier to the training data
voting_classifier.fit(X_train, y_train)

# Making predictions on the test data
y_pred = voting_classifier.predict(X_test)

# Calculating accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Generating and printing the classification report
class_report = classification_report(y_test, y_pred)
print("Classification Report:")
print(class_report)

# Generating and printing the confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)
print("\nConfusion Matrix:")
print(conf_matrix)



import itertools

# Define different classifiers
classifiers = {
    'Random Forest': rf_classifier,
    'Gradient Boosting': gb_classifier,
    'Support Vector Machine': svc_classifier
}

# Generate combinations of classifiers
for r in range(1, len(classifiers) + 1):
    for combo in itertools.combinations(classifiers.items(), r):
        # Create a Voting Classifier with the current combination
        current_classifiers = list(dict(combo).values())
        current_names = list(dict(combo).keys())

        voting_classifier = VotingClassifier(estimators=list(combo), voting='soft')
        voting_classifier.fit(X_train, y_train)
        y_pred = voting_classifier.predict(X_test)

        # Evaluate performance
        accuracy = accuracy_score(y_test, y_pred)
        class_report = classification_report(y_test, y_pred)
        conf_matrix = confusion_matrix(y_test, y_pred)

        # Print results
        print(f"Combination: {', '.join(current_names)}")
        print("Accuracy:", accuracy)
        print("Classification Report:")
        print(class_report)
        print("Confusion Matrix:")
        print(conf_matrix)
        print("---------------------------------------------")

import itertools
import matplotlib.pyplot as plt

# Define different classifiers
classifiers = {
    'Random Forest': rf_classifier,
    'Gradient Boosting': gb_classifier,
    'Support Vector Machine': svc_classifier
}

# Lists to store results
combination_names = []
accuracies = []

# Generate combinations of classifiers
for r in range(1, len(classifiers) + 1):
    for combo in itertools.combinations(classifiers.items(), r):
        # Create a Voting Classifier with the current combination
        current_names = list(dict(combo).keys())
        voting_classifier = VotingClassifier(estimators=list(combo), voting='soft')
        voting_classifier.fit(X_train, y_train)
        y_pred = voting_classifier.predict(X_test)

        # Evaluate performance
        accuracy = accuracy_score(y_test, y_pred)

        # Store results
        combination_names.append(', '.join(current_names))
        accuracies.append(accuracy)

# Plotting the accuracies for different combinations
plt.figure(figsize=(10, 6))
plt.barh(combination_names, accuracies, color='skyblue')
plt.xlabel('Accuracy')
plt.title('Accuracy of Different Classifier Combinations')
plt.gca().invert_yaxis()  # Invert y-axis to have combinations at the top
plt.tight_layout()
plt.show()

import itertools
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# File path to the CSV file
file_path = "/content/drive/MyDrive/stdz_data.csv"

# Load the dataset
data = pd.read_csv(file_path)

# Splitting the data into features (X) and target variable (y)
X = data.drop(columns=['Heart Attack Risk'])  # Features
y = data['Heart Attack Risk']  # Target variable

# Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=80)

# Define different classifiers
classifiers = {
    'Random Forest': RandomForestClassifier(n_estimators=50, random_state=80),
    'Gradient Boosting': GradientBoostingClassifier(n_estimators=50, random_state=80),
    'Support Vector Machine': SVC(kernel='rbf', probability=True, random_state=80),
    'K-Nearest Neighbors': KNeighborsClassifier(),
    'Decision Tree': DecisionTreeClassifier(random_state=80)
}

# Lists to store results
combination_names = []
accuracies = []

# Generate combinations of classifiers
for r in range(1, len(classifiers) + 1):
    for combo in itertools.combinations(classifiers.items(), r):
        # Create a Voting Classifier with the current combination
        current_names = list(dict(combo).keys())
        voting_classifier = VotingClassifier(estimators=list(combo), voting='soft')
        voting_classifier.fit(X_train, y_train)
        y_pred = voting_classifier.predict(X_test)

        # Evaluate performance
        accuracy = accuracy_score(y_test, y_pred)

        # Store results
        combination_names.append(', '.join(current_names))
        accuracies.append(accuracy)

# Plotting the accuracies for different combinations
plt.figure(figsize=(12, 8))
plt.barh(combination_names, accuracies, color='skyblue')
plt.xlabel('Accuracy')
plt.title('Accuracy of Different Classifier Combinations')
plt.gca().invert_yaxis()  # Invert y-axis to have combinations at the top
plt.tight_layout()
plt.show()

# prompt: show percentage in table form fom above  bar chart

# Convert accuracies to percentages
percentages = [round(accuracy * 100, 2) for accuracy in accuracies]

# Create a table with combination names and percentages
table = pd.DataFrame({'Combination': combination_names, 'Accuracy (%)': percentages})

# Print the table
print(table)

import itertools
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# File path to the CSV file
file_path = "/content/drive/MyDrive/stdz_data.csv"

# Load the dataset
data = pd.read_csv(file_path)

# Splitting the data into features (X) and target variable (y)
X = data.drop(columns=['Heart Attack Risk'])  # Features
y = data['Heart Attack Risk']  # Target variable

# Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=80)

# Define different classifiers
classifiers = {
    'Random Forest': RandomForestClassifier(n_estimators=50, random_state=80),
    'Gradient Boosting': GradientBoostingClassifier(n_estimators=50, random_state=80),
    'Support Vector Machine': SVC(kernel='rbf', probability=True, random_state=80),
    'K-Nearest Neighbors': KNeighborsClassifier(),
    'Decision Tree': DecisionTreeClassifier(random_state=80)
}

# Lists to store results
combination_names = []
accuracies = []

# Generate combinations of classifiers
for r in range(1, len(classifiers) + 1):
    for combo in itertools.combinations(classifiers.items(), r):
        # Create a Voting Classifier with the current combination
        current_names = list(dict(combo).keys())
        voting_classifier = VotingClassifier(estimators=list(combo), voting='soft')
        voting_classifier.fit(X_train, y_train)
        y_pred = voting_classifier.predict(X_test)

        # Evaluate performance
        accuracy = accuracy_score(y_test, y_pred)

        # Store results
        combination_names.append(', '.join(current_names))
        accuracies.append(accuracy)

# Plotting the accuracies for different combinations
plt.figure(figsize=(12, 8))
bars = plt.barh(combination_names, accuracies, color='skyblue')
plt.xlabel('Accuracy')
plt.title('Accuracy of Different Classifier Combinations')
plt.gca().invert_yaxis()  # Invert y-axis to have combinations at the top

# Add percentage values on the bars
for bar, acc in zip(bars, accuracies):
    plt.text(bar.get_width() - 0.03, bar.get_y() + bar.get_height() / 2, f'{acc*100:.2f}%',
             va='center', ha='right', color='black')

plt.tight_layout()
plt.show()

# Define SVM classifier
svc_classifier = SVC(kernel='rbf', probability=True, random_state=80)

# Fit and predict using SVM classifier
svc_classifier.fit(X_train, y_train)
svc_pred = svc_classifier.predict(X_test)

# Evaluate SVM performance
svc_accuracy = accuracy_score(y_test, svc_pred)
svc_confusion = confusion_matrix(y_test, svc_pred)

# Display SVM accuracy and confusion matrix
print("SVM Accuracy:", svc_accuracy)
print("SVM Confusion Matrix:")
print(svc_confusion)

# Define Gradient Boosting classifier
gb_classifier = GradientBoostingClassifier(n_estimators=50)

# Fit and predict using Gradient Boosting classifier
gb_classifier.fit(X_train, y_train)
gb_pred = gb_classifier.predict(X_test)

# Evaluate Gradient Boosting performance
gb_accuracy = accuracy_score(y_test, gb_pred)
gb_confusion = confusion_matrix(y_test, gb_pred)

# Display Gradient Boosting accuracy and confusion matrix
print("Gradient Boosting Accuracy:", gb_accuracy)
print("Gradient Boosting Confusion Matrix:")
print(gb_confusion)

# Define Random Forest classifier
rf_classifier = RandomForestClassifier(n_estimators=50)

# Fit and predict using Random Forest classifier
rf_classifier.fit(X_train, y_train)
rf_pred = rf_classifier.predict(X_test)

# Evaluate Random Forest performance
rf_accuracy = accuracy_score(y_test, rf_pred)
rf_confusion = confusion_matrix(y_test, rf_pred)

# Display Random Forest accuracy and confusion matrix
print("Random Forest Accuracy:", rf_accuracy)
print("Random Forest Confusion Matrix:")
print(rf_confusion)

# Define SVM and Gradient Boosting classifiers
svc_classifier = SVC(kernel='rbf', probability=True, random_state=80)
gb_classifier = GradientBoostingClassifier(n_estimators=50, random_state=80)

# Fit and predict using Voting Classifier (SVM + Gradient Boosting)
voting_classifier = VotingClassifier(estimators=[('SVM', svc_classifier), ('Gradient Boosting', gb_classifier)], voting='soft')
voting_classifier.fit(X_train, y_train)
voting_pred = voting_classifier.predict(X_test)

# Evaluate Voting Classifier (SVM + Gradient Boosting) performance
voting_accuracy = accuracy_score(y_test, voting_pred)
voting_confusion = confusion_matrix(y_test, voting_pred)

# Display Voting Classifier accuracy and confusion matrix
print("Voting Classifier (SVM + Gradient Boosting) Accuracy:", voting_accuracy)
print("Voting Classifier (SVM + Gradient Boosting) Confusion Matrix:")
print(voting_confusion)

# Define accuracies and confusion matrices for each classifier/combinations
accuracies = [svc_accuracy, gb_accuracy, rf_accuracy, voting_accuracy]
confusion_matrices = [svc_confusion, gb_confusion, rf_confusion, voting_confusion]
classifiers = ['SVM', 'Gradient Boosting', 'Random Forest', 'SVM + Gradient Boosting']

# Plotting the accuracies for different classifiers/combinations with percentage visualization
plt.figure(figsize=(10, 6))
bars = plt.bar(classifiers, accuracies, color='skyblue')
plt.xlabel('Classifiers/Combinations')
plt.ylabel('Accuracy')
plt.title('Accuracy of Classifiers and Combination')
plt.ylim(0, 1)  # Setting y-axis limit to 0-1 for accuracy
plt.xticks(rotation=45)

# Add percentage values on the bars
for bar, acc in zip(bars, accuracies):
    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f'{acc*100:.2f}%',
             va='bottom', ha='center', color='black')

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

# Define accuracies and confusion matrices for each classifier/combinations
accuracies = [svc_accuracy, gb_accuracy, rf_accuracy, voting_accuracy]
confusion_matrices = [svc_confusion, gb_confusion, rf_confusion, voting_confusion]
classifiers = ['SVM', 'Gradient Boosting', 'Random Forest', 'SVM + Gradient Boosting']

# Define colors for each bar
colors = ['blue', 'blue', 'blue', 'red']

# Plotting the accuracies for different classifiers/combinations with percentage visualization
plt.figure(figsize=(10, 6))
bars = plt.bar(classifiers, accuracies, color=colors)
plt.xlabel('Classifiers/Combinations')
plt.ylabel('Accuracy')
plt.title('Accuracy of Classifiers and Combination')
plt.ylim(0, 1)  # Setting y-axis limit to 0-1 for accuracy
plt.xticks(rotation=45)

# Add percentage values on the bars
for bar, acc in zip(bars, accuracies):
    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f'{acc*100:.2f}%',
             va='bottom', ha='center', color='black')

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Generating random data for demonstration purposes
np.random.seed(42)
X = np.random.rand(50, 2) * 2 - 1  # Random points in 2D space
y = np.where(X[:, 0] + X[:, 1] > 0, 1, -1)  # Classifying based on the line y = -x

# Scatter plot of the data points
plt.figure(figsize=(6, 6))
plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='blue', label='Class 1')
plt.scatter(X[y == -1][:, 0], X[y == -1][:, 1], color='red', label='Class -1')

# Plotting the separating hyperplane
plt.axline((0, 0), slope=1, color='green', linestyle='--', label='Optimal Hyperplane')

plt.xlabel('X1')
plt.ylabel('X2')
plt.title('Support Vector Machines (SVM)')
plt.legend()
plt.grid(True)
plt.show()

from sklearn.datasets import make_circles
from sklearn.ensemble import AdaBoostClassifier
import matplotlib.pyplot as plt

# Generating a synthetic dataset with concentric circles
X, y = make_circles(n_samples=300, noise=0.1, factor=0.5, random_state=42)

# Initialize AdaBoost classifier with DecisionTree base estimator
adaboost = AdaBoostClassifier(n_estimators=4, random_state=42)
plt.figure(figsize=(12, 8))

# Sequentially fit and plot decision boundaries
for i in range(4):
    plt.subplot(2, 2, i+1)
    adaboost.fit(X, y)

    # Plotting decision boundary
    xx, yy = np.meshgrid(np.linspace(-1.5, 1.5, 100), np.linspace(-1.5, 1.5, 100))
    Z = adaboost.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)
    plt.contourf(xx, yy, Z, cmap=plt.cm.RdBu, alpha=0.8)

    # Plotting training points
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdBu, edgecolors='k')
    plt.title(f"Iteration {i+1}")

plt.tight_layout()
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeRegressor

# Generate synthetic data with noise
np.random.seed(42)
X = np.linspace(0, 10, 100)
y = np.sin(X) + np.random.normal(0, 0.3, size=X.shape[0])

# Initialize predictions with zeros
y_pred = np.zeros_like(y)

# Number of iterations (number of trees to be added)
n_estimators = 5

plt.figure(figsize=(10, 8))
plt.scatter(X, y, color='blue', label='Actual')

# Gradient Boosting iterations
for i in range(n_estimators):
    # Fit a decision tree regressor
    tree = DecisionTreeRegressor(max_depth=2, random_state=42)
    tree.fit(X.reshape(-1, 1), y - y_pred)  # Fit on residuals (errors)

    # Make predictions using the current tree
    y_pred_i = tree.predict(X.reshape(-1, 1))

    # Update predictions with the new tree's predictions
    y_pred += y_pred_i

    # Plotting each iteration's prediction
    if i == 0:
        plt.plot(X, y_pred, color='orange', alpha=0.7, label='GB Predictions')
    else:
        plt.plot(X, y_pred, color='orange', alpha=0.7)

    # Update residuals for the next iteration
    y = y - y_pred

plt.xlabel('X')
plt.ylabel('y')
plt.title('Gradient Boosting Visualization')
plt.legend()
plt.grid(True)
plt.show()



